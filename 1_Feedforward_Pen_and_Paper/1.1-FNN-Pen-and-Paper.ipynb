{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7yQ-xkp8Ed_"
   },
   "source": [
    "# 02456 Deep Learning Exercise 1 Pen and Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIYay8oI8EeA"
   },
   "source": [
    "# Contents and why we need this lab\n",
    "\n",
    "The first lab is all about understanding the mathematical concepts behind neural networks. You will derive everything by hand this first week. The hand-in this week is your modifications to this notebook.\n",
    "\n",
    "In week two you will program what you have derived this week in  [NumPy](https://numpy.org/) and in week three and onwards you will work in a framework dedicated to making it easy to write deep learning code. In this course we will use [PyTorch](https://pytorch.org/) because it is more Python-like than for example TensorFlow. However, TensorFlow has some advantages if you want to deploy deep learning models. For learning and research PyTorch is right now the preferred framework. But this is a quite dynamic field where a lot is happening, so it is hard to say what is the preferred framework a year from now.    \n",
    "\n",
    "Linear algebra, probability theory, statistics and optimization are all underpinning deep learning. The availability of coding frameworks for machine learning is to some degree hiding this and making it possible to make quite impressive applications without deep understanding of the underlying mathematical concepts. This is both a blessing and a curse. The ambition of this course is to educate first class deep learners that can develop deep learning solutions tailored to the problem at hand. Therefore we need the fundamental understanding. We have experienced that neglecting the fundamental will always come back to hunt you later when we encounter real world problems. So sharpen your pen and get ready to learn new stuff or more likely refreshen some stuff you forgot you knew years ago. :-) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-99wevZp8EeB"
   },
   "source": [
    "# List of contents\n",
    "1. External sources of information\n",
    "2. Neural networks - the feed-forward model\n",
    "3. Loss functions and maximum likelihood\n",
    "4. Stochastic gradient descent\n",
    "5. Error backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgiYEvI98EeB"
   },
   "source": [
    "# External sources of information\n",
    "\n",
    "1. Book. The notation will to a high degree follow that of [Chris Bishop, Pattern recognition and machine learning](https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book). This book is freely available online and is still a very valuable source of information of all things machine learning.\n",
    "2. Jupyter notebook. You can find more information about Jupyter notebooks [here](https://jupyter.org/). It will come as part of the [Anaconda](https://www.anaconda.com/) Python installation. \n",
    "3. Markdown. Jupyter notebook's uses cells. A cell is executed by pressing the Run tab above. In the lab you will only use Markdown cells. You add cell by pressing + tab above of choosing it from the Insert menu also above. A good overview of Markdown formatting can be found [here](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). For equations Markdown uses [latex](https://en.wikibooks.org/wiki/LaTeX/Mathematics).\n",
    "4. [Mathematics for machine learning](https://mml-book.github.io/book/mml-book.pdf) is a book that where the title says it all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUoyG50y8EeC"
   },
   "source": [
    "# Neural networks - the feed forward model\n",
    "We will meet different variants of neural network models throughout the course. We need different architectures because they have different type of symmetries that reflect data we encounter with differnt spatial and temporal invariances. \n",
    "\n",
    "To get started we work with the most basic variant: the so-called *feed-forward neural network* (FFNN). A FFNN with one hidden layer with $M$ hidden unit and two layers of adaptable parameters (weights) denoted by $w=w^{(1)},w^{(2)}$ is plotted below. In literature sometimes $\\theta$ will be used instead of $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwTZDCXs8EeC",
    "outputId": "4e9a7667-f21b-43b1-cb70-3710de011b5b"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.13 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.9 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Figure 5.1 from Bishop\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"figures/Figure5.1.jpg\",width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zFgqJyV98EeG"
   },
   "source": [
    "We are considering supervised learning where we both have inputs (covariates) and assocatiated outputs (labels, targets, response variables) available. The network has $D$ inputs: $\\mathbf{x}=x_1,\\ldots,x_D$ and $K$ outputs $\\mathbf{y}=y_1,\\ldots,y_K$. The algorithm is called a neural network because the architecture of the model resembles the biological network between neurons in our brains. \n",
    "\n",
    "This computation is layered. Each layer first computes what is equivalent to the output $a_j$ of a linear statistical model and then applies a non-linear function to the linear model output. For the first layer - which takes the network input $x$ as input - these two steps look like this\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{(1)}_j & = \\sum_{i=1}^D w^{(1)}_{ji} x_i + w^{(1)}_{j0} \\\\\n",
    "z^{(1)}_j & = h_1(a^{(1)}_j ) \\ ,\n",
    "\\end{align}\n",
    "$$\n",
    "where $h_1$ is the non-linear function in the first layer. We can get rid of writing the so-called bias $w^{(1)}_{j0}$ explicitly by adding an extra input $x_0$ which is always set to one and extending the sum to go from zero:\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{(1)}_j & = \\sum_{i=0}^D w^{(1)}_{ji} x_i \\ . \n",
    "\\end{align}\n",
    "$$\n",
    "We will use this notation in the following for all layers and just write for example $\\sum_i \\ldots$ to be understood as $\\sum_{i=0}^D \\ldots$. \n",
    "\n",
    "The second layer takes the output the of the first layer as input:  \n",
    "$$\n",
    "a^{(2)}_j = \\sum_{i}^M w^{(2)}_{ji} z^{(1)}_i \\ . \n",
    "$$\n",
    "The second layer non-linear function is denoted by $h_2$ so the output of the network is\n",
    "$$\n",
    "y_j = h_2(a^{(2)}_j)\n",
    "$$\n",
    "This gives an example of how the neural network model input to output mapping can be specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qSUaU0F38EeH"
   },
   "source": [
    "Let us do a few exercise where you complete the equations by replacing $\\ldots$ with the correct expressions.\n",
    "\n",
    "## Exercise a)\n",
    "\n",
    "a) Write $y_j$ directly as a function of $x$. That is, eliminate the $a$s and $z$s:\n",
    "\n",
    "$$\n",
    "y_j = h_2(\\ldots)\n",
    "$$\n",
    "1:\n",
    "$$\n",
    "y_j = h_2(a^{(2)}_j)\n",
    "$$\n",
    "2:\n",
    "$$\n",
    "y_j = h_2(\\sum_{i}^M w^{(2)}_{ji} z^{(1)}_i)\n",
    "$$\n",
    "3:\n",
    "$$\n",
    "y_j = h_2(\\sum_{i}^M w^{(2)}_{ji} h_1(a^{(1)}_i ))\n",
    "$$\n",
    "4:\n",
    "$$\n",
    "y_j = h_2(\\sum_{i=1}^M w^{(2)}_{ji} h_1(\\sum_{k=0}^D w^{(1)}_{ik} x_k ))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bIXe3rGW8EeH"
   },
   "source": [
    "## Exercise b)\n",
    "\n",
    "b) Write the equation for a neural network with two hidden layers and three layers of weights $w=w^{(1)},w^{(2)},w^{(3)}$. Again, without using $a$s and $z$s.\n",
    "$$\n",
    "y_j = h_3(\\ldots)\n",
    "$$\n",
    "$$\n",
    "y_j = h_3(\\sum_{i}^W w^{(3)}_{ji} z^{(2)}_i)\n",
    "$$\n",
    "$$\n",
    "y_j = h_3(\\sum_{i=1}^W w^{(3)}_{ji} h_2(a^{(2)}_i))\n",
    "$$\n",
    "$$\n",
    "y_j = h_3(\\sum_{i=1}^W w^{(3)}_{ji} h_2(\\sum_{k=1}^M w^{(2)}_{ik} z^{(1)}_k))\n",
    "$$\n",
    "$$\n",
    "y_j = h_3(\\sum_{i=1}^W w^{(3)}_{ji} h_2(\\sum_{k=1}^M w^{(2)}_{ik} h_1(a^{(1)}_k)))\n",
    "$$\n",
    "$$\n",
    "y_j = h_3(\\sum_{i=1}^W w^{(3)}_{ji} h_2(\\sum_{k=1}^M w^{(2)}_{ik} h_1(\\sum_{h=0}^D w^{(1)}_{kh} x_h)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1s_iYsJ8EeH"
   },
   "source": [
    "## Exercise c)\n",
    "\n",
    "c) Write the equations for an FFNN with $L$ layers as recursion. Use $l$ as the index for the layer:\n",
    "$$\n",
    "\\begin{align}\n",
    "y_j & = h_L(\\sum_{i}^D w^{L}_{ji} z^{(L-1)}_i) & \\\\\n",
    "z^{(l)}_j & = h_l(a^{(l)}_j) & l=1,\\ldots,L \\\\\n",
    "a^{(l)}_j & = \\sum_{i=1}^D w^{(l)}_{ji} z^{(l-1)}_is &  l=2,\\ldots,L \\\\\n",
    "a^{(1)}_j & = \\sum_{i=0}^D w^{(1)}_{ji} x_i & \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aos_YR0Y8EeI"
   },
   "source": [
    "## Exercise d) optional\n",
    "\n",
    "Optional exercises do not give extra credits but are included to give you an opportunity to dive a bit deeper. You can omit them for the first run through the lab and then return to them later on when the other exercises are completed.\n",
    "\n",
    "d) Do we really need the non-linearities? Show that if we remove the non-linear functions $h_l$ from the expressions above then the output becomes linear in $x$. This means that the model collapses back to the linear model and therefore cannot learn non-linear relations between $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q5C36UV68EeI"
   },
   "source": [
    "# Loss functions and maximum likelihood\n",
    "\n",
    "The FFNN model is quite flexible and can learn to approximate all sorts of functions from training data. Below are some examples of one-dimensional functions learned with a FFNN - i.e. we  try to learn a function (the neural network) from a set of points $(x,y)$ that can predict $y$ from input $x$ with as high accuracy as possible. The FFNN we use below consists of one hidden layer with three hidden units, $\\tanh$ as the non-linear function in the hidden layer and linear output:\n",
    "$$\n",
    "y(x) = \\sum_{i=0}^3 w^{(2)}_i \\tanh( w^{(1)}_i x ) \n",
    "$$\n",
    "The traning data is visualised as the blue dots, the learned function $y(x)$ (only displayed in the interval where we have training data) is the full red line and the dashed lines are the output from the hidden units. \n",
    "\n",
    "A very nice website to get intuition about how neural networks fit to data is the [TensorFlow playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4&seed=0.48021&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=regression&initZero=false&hideText=false). The example shown is two dimensional regression: the input $x$ is two-dimensional and the output $y$ is one-dimensional and continuous. Blue encodes high values and yellow low values. You can change many things including switching to classification tasks in the top menu to the right and to other more challenging problems to the left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpKe1K4r8EeJ",
    "outputId": "25ad352a-07bc-4edd-e7fb-88f63a2aadce"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.13 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.9 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Figure 5.3 from Bishop\n",
    "\n",
    "from IPython.display import display\n",
    "display(Image(\"figures/Figure5.3a.jpg\",width=400),\\\n",
    "        Image(\"figures/Figure5.3b.jpg\",width=400),\\\n",
    "        Image(\"figures/Figure5.3c.jpg\",width=400),\\\n",
    "        Image(\"figures/Figure5.3d.jpg\",width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wY1Agi88EeL"
   },
   "source": [
    "This gives some insight into how the neural network uses the hidden units to approximate the curve. But we need to formalise learning and fitting to data. Here the concept of a loss/error function and later on maximum likelihood will come in handy. \n",
    "\n",
    "The training set $\\mathcal{D}=\\{ (\\mathbf{x}_n,\\mathbf{t}_n)|n=1,\\ldots,N\\}$ is comprised of $N$ input-output pairs. In the plots above these are all the blue dots. \n",
    "\n",
    "We can define a loss or error function $E(w)$ as some measure of how far $y(x)$ is from the training data, that is the difference between the red line and the blue dots. As above, $w$ denotes the set of weights of the FFNN, which are the parameters of the neural network we are trying to learn - i.e. we wish to determine a set of weights that results in a low value of the loss function. This is the training error as it is only computed for the training points. In literature sometimes $L$ or $J$ will be used instead of $E$.\n",
    "\n",
    "In machine learning we are very much concerned with what the model predicts for inputs which are not in the training set. We often talk about the generalisation error which measures the loss function on data points sampled randomly from the data distribution. In general we do not know the data distribution so instead we set aside a subset of the data we have - the test set - to compute an estimate of the generalisation error. An important skill to have in machine learning is to perform the optimization of the model in such a way that it generalizes well. Much more about that in the coming weeks...\n",
    "\n",
    "An example of a loss function is the sum of squares:\n",
    "$$\n",
    "E(w) = \\sum_{n=1}^N || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 \\ ,\n",
    "$$\n",
    "where $|| \\ldots ||_2^2$ is shorthand for the sum of squared components.\n",
    "\n",
    "This sum of squares error function is appropiate for regression (= targets are continuous) as in the example above. It is not useful for classification where targets are discrete class labels. So in order to come up with a proper loss function for all cases we will appeal to the maximum likelihood. The likelihood is defined as the probability of the data - in supervised learning the target $\\mathbf{t}_1,\\ldots,\\mathbf{t}_N$ - given the model parameters and the inputs:\n",
    "$$\n",
    "p(\\mathbf{t}_1,\\ldots,\\mathbf{t}_N|\\mathbf{x}_1,\\ldots,\\mathbf{x}_N,w) \\ .\n",
    "$$\n",
    "Maximum likelihood simply says that we should use the set of parameters $w$ that assigns the highest possible probability to the observed targets $\\mathbf{t}_1,\\ldots,\\mathbf{t}_N$.\n",
    "\n",
    "In order to go further from here we need to make some assumptions:\n",
    "\n",
    "1. Unless we are working with series data - such as time series - we will assume that each data point is independently sampled and each data point is sampled from the same distribution (also known as iid = Independent and identically distributed):\n",
    "$$\n",
    "p(\\mathbf{t}_1,\\ldots,\\mathbf{t}_N|\\mathbf{x}_1,\\ldots,\\mathbf{x}_N,w) =\n",
    "\\prod_{n=1}^N p(\\mathbf{t}_n|\\mathbf{x}_n,w) \\ .\n",
    "$$\n",
    "2. To connect with the sum of squares loss we will assume that the target can be written as the model output plus some random error $\\mathbf{\\epsilon}$ and that $\\mathbf{\\epsilon}$ has a Gaussian distribution with zero mean and covariance $\\sigma^2 \\mathbf{I}$. This means that targets themselves are Gaussian distributed with mean $\\mathbf{y}(\\mathbf{x})$ and covariance $\\sigma^2 \\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix. Denoting the Gaussian distribution by $\\mathcal{N}$ we can write this as:\n",
    "$$\n",
    "p(\\mathbf{t}_n|\\mathbf{x}_n,w) = \\mathcal{N}(\\mathbf{t}_n|\\mathbf{y}(\\mathbf{x}_n),\\sigma^2 \\mathbf{I}) \\ . \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGW8GSoW8EeL"
   },
   "source": [
    "## Exercise e)\n",
    "\n",
    "e) In this exercise you will show that with the two above assumptions we can derive a loss function that contains $E(w)$ as a term. Two hints\n",
    "\n",
    "1. With the used covariance we can write the Gaussian distribution as\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{t}_n|\\mathbf{y}(\\mathbf{x}_n),\\sigma^2 \\mathbf{I}) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}^D}\n",
    "\\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 )\n",
    "$$\n",
    "2. In order to turn maximum likelihood into a loss/error function apply the (natural) logarithm to the likelihood objective and multiply by minus one.\n",
    "\n",
    "Show that the loss we get is\n",
    "$$\n",
    "\\frac{ND}{2} \\log 2\\pi \\sigma^2 + \\frac{1}{2\\sigma^2} E(w) \\ . \n",
    "$$\n",
    "SOLUTION:\\\n",
    "Given the two assumptions and the first hint we write the expression:\n",
    "$$\n",
    "p(\\mathbf{t}_1,\\ldots,\\mathbf{t}_N|\\mathbf{x}_1,\\ldots,\\mathbf{x}_N,w) =\n",
    "\\prod_{n=1}^N p(\\mathbf{t}_n|\\mathbf{x}_n,w) =\\prod_{n=1}^N \\frac{1}{\\sqrt{2\\pi \\sigma^2}^D}\n",
    "\\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 ) \\\n",
    "$$\n",
    "\n",
    "We take the natural log and multiply by negative zero:\n",
    "$$\n",
    "-nlog(\\prod_{n=1}^N p(\\mathbf{t}_n|\\mathbf{x}_n,w)) = -nlog(\\prod_{n=1}^N \\frac{1}{\\sqrt{2\\pi \\sigma^2}^D}\n",
    "\\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 )) \\\n",
    "$$\n",
    "We simplify:\\\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "-log\\prod_{n=1}^N p(\\mathbf{t}_n|\\mathbf{x}_n,w)) & = -log(\\prod_{n=1}^N \\frac{1}{\\sqrt{2\\pi \\sigma^2}^D}\n",
    "\\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 )) \\\\\n",
    "& = -\\sum_{n=1}^N log(\\frac{1}{\\sqrt{2\\pi \\sigma^2}^D} \\exp( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 )) \\\\\n",
    "& = -\\sum_{n=1}^N ( log(\\frac{1}{\\sqrt{2\\pi \\sigma^2}^D}) - log(\\exp( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 ))) \\\\\n",
    "& = -\\sum_{n=1}^N ( log(\\frac{1}{\\sqrt{2\\pi \\sigma^2}^D}) - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 ) \\\\\n",
    "& = \\sum_{n=1}^N( -log(\\frac{1}{\\sqrt{2\\pi \\sigma^2}^D}) + || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 ) \\\\\n",
    "& = \\sum_{n=1}^N( -(log(1) - log(\\sqrt{2\\pi \\sigma^2}^D)) + || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 ) \\\\\n",
    "& = \\sum_{n=1}^N( -( 0 - log(\\sqrt{2\\pi \\sigma^2}^D)) + || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 ) \\\\\n",
    "& = \\sum_{n=1}^N( log((\\pi \\sigma^2)^{0.5})D + || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 ) \\\\\n",
    "& =  \\sum_{n=1}^N \\frac{1}{2}log(\\pi \\sigma^2)D + \\sum_{n=1}^N|| \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2  \\\\\n",
    "& =   \\frac{ND}{2}log(\\pi \\sigma^2) + E(w) /2\\sigma^2  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "We see that we derive at the original expression:\n",
    "\n",
    "$$\n",
    "\\frac{ND}{2} \\log 2\\pi \\sigma^2 + \\frac{1}{2\\sigma^2} E(w) \\ . \n",
    "$$\n",
    "\n",
    "Further argue why applying the log and multiplying by minus one is the right thing to do in order to get a loss function. *Hint:* Will the optimum of the likelihood function change if we apply the logarithm?\n",
    "\n",
    "The likehood is within the range of [0,1], if we take the log of the likelihood we arrive at an expression were lower probabilities result in more negative losses. Multiplying by -1 results in high losses for bad prediction w.r.t. the target or ground truth and low losses for high probabilities. The log-likelihood function is great for training the model to predict labels, as it learns to predict 1 the ground true label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sjk_rzBK8EeL"
   },
   "source": [
    "## Exercise f) optional\n",
    "\n",
    "f) Show that the optimum (= minimum of the loss) with respect to $w$ is not affected by the value of $\\sigma^2$. Find the optimum for $\\sigma^2$ as a function of $w$. \n",
    "\n",
    "This means that for the problem of finding $w$, sum of squares and maximum likelihood for the Gaussian likelihood with $\\sigma^2 \\mathbf{I}$ covariance are equivalant.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sP0NU5Yt8EeM"
   },
   "source": [
    "## Classification, one-hot and softmax\n",
    "\n",
    "We will now turn to classification. In classication the $K$-dimensional target vector $\\mathbf{t}$ encodes exactly one out of $K$ possible classes. It is convenient to use the so-called one-hot encoding for this meaning that the $\\mathbf{t}$ vector will have $K-1$ zeros and a single one at position $k$ if the target for the data point is class $k$. For example, if we have $K=4$ and the correct label is class three then $\\mathbf{t}=(0,0,1,0)$.\n",
    "\n",
    "We need to modify the network output $\\mathbf{y}(\\mathbf{x})$ in order to get a likelihood function. In the example above the likelihood should be the probability that the model assigns to class three. This means that the output should be probabilities. We can achieve this with the softmax function:\n",
    "$$\n",
    "y_k = \\frac{\\exp ( a_k )}{\\sum_j \\exp ( a_j )} \\ ,\n",
    "$$\n",
    "where $a_j$ is shorthand for the linear output from the last layer: $a^{(L)}_j$. The $a$ vector is what in statistics is called the multinomial link function and they are also called the logits. Combining the one-hot encoding of the target with the probabilistic model outputs we can write:\n",
    "$$\n",
    "p(\\mathbf{t}_n|\\mathbf{x}_n,w) = \\prod_{k=1}^K \\left[ y_k(\\mathbf{x}_n)\\right]^{t_{nk}} \\ .\n",
    "$$\n",
    "We can now see that the one-hot encoding selects exactly the right output term and the other terms with $t_{nj}=0$ with contribute ones because $[y_{nj}]^0=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qnNH2wO8EeM"
   },
   "source": [
    "## Exercise g) \n",
    "\n",
    "g) Show using the same procedure we used for regression that the loss function for classification is: \n",
    "$$\n",
    "E(w) = - \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log y_{k}(\\mathbf{x}_n) \\ .\n",
    "$$\n",
    "\n",
    "SOLUTION:\n",
    "Recall iid:\n",
    "$$\n",
    "p(\\mathbf{t}_1,\\ldots,\\mathbf{t}_N|\\mathbf{x}_1,\\ldots,\\mathbf{x}_N,w) =\n",
    "\\prod_{n=1}^N p(\\mathbf{t}_n|\\mathbf{x}_n,w) \\ .\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-log p(\\mathbf{t}_1,\\ldots,\\mathbf{t}_N|\\mathbf{x}_1,\\ldots,\\mathbf{x}_N,w) & = -log \\prod_{n=1}^N \\prod_{k=1}^K  \\left[ y_k(\\mathbf{x}_n)\\right]^{t_{nk}} \\\\\n",
    "& =-\\sum_{n=1}^N\\sum_{k=1}^K  log \\left[ y_k(\\mathbf{x}_n)\\right]^{t_{nk}} \\\\\n",
    "& =-\\sum_{n=1}^N\\sum_{k=1}^K  t_{nk}log \\left[ y_k(\\mathbf{x}_n)\\right] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is also known as the cross entropy loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ogjpXzBl8EeM"
   },
   "source": [
    "# Stochastic gradient descent\n",
    "\n",
    "To paraphrase Stan Lee: With great flexibility comes complicated fitting to data! We therefore have to resort to general purpose optimization. Optimization refers to the process of searching for a set of weights $w$ that achieves some good results, such as a low loss on the training set. The loss function is differentiable so we can use gradient information to guide our search. In deep learning some variant of stochastic gradient descent is almost always used for optimization when the gradients can be computed. Stochastic here means that the gradient is computed on a subset of the training set.\n",
    "\n",
    "Basic gradient descent means that we take a step with step-size $\\eta$ opposite the parameter gradient direction:\n",
    "$$\n",
    "w^{\\mathrm{new}} = w - \\eta \\nabla E(w) \\ , \n",
    "$$\n",
    "where $\\nabla$ is the parameter gradient operator. Applying $\\nabla$ to a scalar function like $E(w)$ will produce a vector as output where the $j$th component of the vector is the derivative of $E(w)$ with respect to the $j$th parameter. A conceptional sketch of the optimization problem is given in the figure below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msN_jFr78EeN",
    "outputId": "b9d51082-45a1-4260-8ec5-3ae8f165408c"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.13 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.9 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Bishop figure 5.5 \n",
    "\n",
    "Image(\"figures/Figure5.5.jpg\",width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pwskct8B8EeP"
   },
   "source": [
    "## Exercise h) optional\n",
    "Prove that the gradient calculated on a random subset of the training set on average is proportional to the true gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3taSF0B8EeP"
   },
   "source": [
    "# Error backpropagation\n",
    "So-called error backpropagation is simply a recipe for calculating gradients of layered models such as neural networks. Efficient computation is based upon 1) the [chain-rule of differentiation](https://en.wikipedia.org/wiki/Chain_rule):\n",
    "$$\n",
    "\\frac{\\partial f(g(w))}{\\partial w} = \\left. \\frac{\\partial f(g)}{\\partial g} \\right|_{g=g(w)} \\frac{\\partial g(w)}{\\partial w}\n",
    "$$\n",
    "and 2) storing intermediate computations. In the following we will omit writing $g=g(w)$ and instead let it be understood from the context.\n",
    "\n",
    "## Gradient for layer $L$\n",
    "\n",
    "We return to the FFNN with $L$ layers. We start by computing the gradient with respect to a weight in the last layer:\n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w^{(L)}_{ji}} \\ .\n",
    "$$\n",
    "To make life a bit simpler for ourselves we will assume that our training set consists of one example so we can drop the training point index $n$. (As an optional exercise below you can put the summation over the training examples back in.)\n",
    "\n",
    "First we observe the $w$ dependence in $E(w)$ is through $a^{(L)}_1,\\ldots,a^{(L)}_K$ so we can now apply the chain-rule\n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w^{(L)}_{ji}} = \\sum_{k=1}^K \\frac{\\partial E(w)}{\\partial a^{(L)}_{k}} \\frac{\\partial a^{(L)}_{k}}{\\partial w^{(L)}_{ji}} \\ .\n",
    "$$\n",
    "We can use that $a^{(L)}_{k} = \\sum_i w^{(L)}_{ki} z^{(L-1)}_i$ to conclude that $\\frac{\\partial a^{(L)}_{k}}{\\partial w^{(L)}_{ji}} = z^{(L-1)}_i$ when $j=k$ and zero otherwise. So we can write \n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w^{(L)}_{ji}} = \\delta^{(L)}_j  z^{(L-1)}_i \\ ,\n",
    "$$\n",
    "where we with foresight have defined \n",
    "$$\n",
    "\\delta^{(L)}_j = \\frac{\\partial E(w)}{\\partial a^{(L)}_{j}} \\ .\n",
    "$$\n",
    "These $\\delta$s will become very practical for bookkeeping purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zm9J0lJO8EeP"
   },
   "source": [
    "## Exercise i) \n",
    "Calculate \n",
    "$$\n",
    "\\delta^{(L)}_j = \\frac{\\partial E(w)}{\\partial a^{(L)}_{j}}\n",
    "$$\n",
    "for classification. \n",
    "\n",
    "Hint: It is much easier to find the derivative if we write the loss function directly in terms of the logits $a^{(L)}_{j}$. So show first using the definition of the loss and the softmax that \n",
    "$$\n",
    "E(w) = - \\sum_{k=1}^K t_k a^{(L)}_{k} + \\log \\sum_{k=1}^K \\exp( a^{(L)}_{k} ) \\ . \n",
    "$$\n",
    "Finally show that \n",
    "$$\n",
    "\\frac{\\partial}{\\partial a^{(L)}_{j}} \\log \\sum_{k=1}^K \\exp( a^{(L)}_{k} ) = y_j\n",
    "$$\n",
    "to get the final result.\n",
    "\n",
    "SOLUTION:\n",
    "\n",
    "Recall the following:\n",
    "\n",
    "$$\n",
    "y_k = \\frac{\\exp ( a_k )}{\\sum_j \\exp ( a_j )} \\\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E(w) = -\\sum_{k=1}^K log \\left[ y_k \\right]  &=  -\\sum_{k=1}^K  t_{k}log \\left[ \\frac{\\exp ( a_k )}{\\sum_{j=1}^J \\exp ( a_j )} \\right]\\\\\n",
    "&=  -\\sum_{k=1}^K  t_{k} \\left[ log \\exp( a_k ) - log \\sum_{j=1}^J \\exp ( a_j ) \\right]\\\\\n",
    "&=  -\\sum_{k=1}^K  t_{k} \\left[a_k  - log \\sum_{j=1}^J \\exp ( a_j ) \\right]\\\\\n",
    "&=  - \\left [ \\sum_{k=1}^K t_{k}a_k  - \\sum_{k=1}^K t_{k} log \\sum_{j=1}^J \\exp ( a_j ) \\right]\\\\\n",
    "&=  - \\left [ \\sum_{k=1}^K t_{k}a_k  - log \\sum_{j=1}^J \\exp ( a_j ) \\right]\\\\\n",
    "&=  -\\sum_{k=1}^K t_{k}a_k  - log \\sum_{j=1}^J \\exp ( a_j )\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since K = J, we can write:\n",
    "\n",
    "$$\n",
    "E(w) = -\\sum_{k=1}^K t_{k}a_k  - log \\sum_{j=1}^J \\exp ( a_j ) = - \\sum_{k=1}^K t_k a^{(L)}_{k} + \\log \\sum_{k=1}^K \\exp( a^{(L)}_{k} )\n",
    "$$\n",
    "\n",
    "Next step:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial a^{(L)}_{j}} \\log \\sum_{k=1}^K \\exp( a^{(L)}_{k} ) &=  \\frac{1} {\\sum_{k=1}^K \\exp( a^{(L)}_{k})}* \\frac{\\partial}{\\partial a^{(L)}_{j}}\\sum_{k=1}^K \\exp( a^{(L)}_{k} ) \\\\\n",
    "&=  \\frac{1} {\\sum_{k=1}^K \\exp( a^{(L)}_{k})} \\exp( a^{(L)}_{j} ) \\\\\n",
    "&=  \\frac{\\exp( a^{(L)}_{j} )} {\\sum_{k=1}^K \\exp( a^{(L)}_{k})}  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "A combination of the chain-rule, seeing that J = K, and the property of a_j being a constant when j!=k, we get the softmax function:\n",
    "\n",
    "$$\n",
    "y_j = \\frac{\\exp( a^{(L)}_{j} )} {\\sum_{k=1}^K \\exp( a^{(L)}_{k})}\n",
    "$$\n",
    "\n",
    "next step\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(L)}_j &= \\frac{\\partial E(w)}{\\partial a^{(L)}_{j}} \\\\\n",
    "&= \\frac{\\partial}{\\partial a^{(L)}_{j}} \\left[- \\sum_{k=1}^K t_k a^{(L)}_{k} + \\log \\sum_{k=1}^K \\exp( a^{(L)}_{k})\\right] \\\\\n",
    "&= \\frac{\\partial}{\\partial a^{(L)}_{j}} \\left[- \\sum_{k=1}^K t_k a^{(L)}_{k}\\right] + \\frac{\\exp( a^{(L)}_{j} )} {\\sum_{k=1}^K \\exp( a^{(L)}_{k})} \\\\\n",
    "&= -  t_j  + \\frac{\\exp( a^{(L)}_{j} )} {\\sum_{k=1}^K \\exp( a^{(L)}_{k})} \\\\\n",
    "&= \\frac{\\exp( a^{(L)}_{j} )} {\\sum_{k=1}^K \\exp( a^{(L)}_{k})} - t_j \\\\\n",
    "&= y_j - t_j \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rP69UTWV8EeQ"
   },
   "source": [
    "## Gradient for layer $L-1$\n",
    "\n",
    "Now we proceed to the second last layer\n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w^{(L-1)}_{ji}} \\ .\n",
    "$$\n",
    "This will allow us to identify a pattern that will enable us to generalize to any layer. \n",
    "\n",
    "We now use that the model is layered so the chain-rule we need is:\n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w^{(L-1)}_{ji}} = \\sum_{k=1}^K \\frac{\\partial E(w)}{\\partial a^{(L)}_{k}} \\frac{\\partial a^{(L)}_{k}}{\\partial a^{(L-1)}_{j}} \\frac{\\partial a^{(L-1)}_{j}}{\\partial w^{(L-1)}_{ji}} \\ .\n",
    "$$\n",
    "We can now again use $a^{(L)}_{k} = \\sum_i w^{(L)}_{ki} z^{(L-1)}_i= \\sum_i w^{(L)}_{ki} h_{L-1}(a^{(L-1)}_i)$ and the definition of $\\delta^{(L)}_j$: \n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w^{(L-1)}_{ji}} = \\sum_{k=1}^K \\delta^{(L)}_k  w^{(L)}_{kj} h_{L-1}'(a^{(L-1)}_j) z^{(L-2)}_i \\ .\n",
    "$$\n",
    "We can now define\n",
    "$$\n",
    "\\delta^{(L-1)}_j = \\sum_{k=1}^K \\delta^{(L)}_k  w^{(L)}_{kj} h_{L-1}'(a^{(L-1)}_j) \\ .\n",
    "$$\n",
    "To write the gradient in the same way as for layer $L$\n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w^{(L-1)}_{ji}} = \\delta^{(L-1)}_j \\ z^{(L-2)}_i \\ . \n",
    "$$\n",
    "If we look a bit closer at the definition of $\\delta^{(L-1)}_j$ we see it is actually nothing but: \n",
    "$$\n",
    "\\delta^{(L-1)}_j = \\frac{\\partial E(w)}{\\partial a^{(L-1)}_{j}} \\ .\n",
    "$$\n",
    "You will use this in the next exercise to derive the backpropagation rule for a general layer $l<L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z0nMwYvA8EeQ"
   },
   "source": [
    "## Exercise j) - the backpropagation rule for layer $l<L$\n",
    "\n",
    "j) Use the above results to argue why the general backpropagation rule for $l<L$ is written as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E(w)}{\\partial w^{(l)}_{ji}} & = \\delta^{(l)}_j  z^{(l-1)}_i \\\\\n",
    "\\delta^{(l)}_j & = \\sum_{k=1}^K \\delta^{(l+1)}_k  w^{(l+1)}_{kj} h_{l}'(a^{(l)}_j) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ANSWER:\n",
    "\n",
    "The above expressions show that, during back-propagation, computing the error $\\delta^{l}_j$ simply requires having computed all $\\delta^{l+1}_k$ for k = 1,...,K. Seeing that errors are already being computed at layer L, these expression can be used to compute errors from layer l = L - 1, ..., 2 (Neurons of layer 1 do not have non-linear activation as inputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ZSQA-7L8EeR"
   },
   "source": [
    "## Exercise k) - optional\n",
    "k) Derive the backpropagation rule for regression. That is perform the calculation in exercise i) for the regression loss. Everything else stays the same. Actually it turns out that is you use $h_L(a) = a$ then you even get the same result as in i)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1NJHrP48EeR"
   },
   "source": [
    "## Exercise l)\n",
    "l) Modify the backpropagation rules to be able to take a dataset of size greater than one. \n",
    "Hint: This only requires introducing a sum over $n$ and $n$ indices in a few places.\n",
    "\n",
    "__Gradient for layer $L$}__\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w^{(L)}_{ji}} = \n",
    "\\frac{1}{N}\\sum^N_{n=1}\\sum_{k=1}^K \\frac{\\partial E(w)}{\\partial a^{(L)}_{nk}} \\frac{\\partial a^{(L)}_{nk}}{\\partial w^{(L)}_{ji}} \\ .\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w^{(L)}_{ji}} = \n",
    "\\frac{1}{N}\\sum^N_{n=1}\\delta^{(L)}_j  z^{(L-1)}_{ni} \\ ,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta^{(L)}_{nj} = \\frac{\\partial E(w)}{\\partial a^{(L)}_{nj}} \\ .\n",
    "$$\n",
    "\n",
    "__Gradient for layer $L-1$}__\n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w^{(l)}_{ji}} = \n",
    "\\frac{1}{N}\\sum^N_{n=1}\\delta^{(l)}_{nj}  z^{(l-1)}_{ni} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta^{(l)}_{nj} = \\sum_{k=1}^K \\delta^{(l+1)}_{nk}  w^{(l+1)}_{kj} h_{l}'(a^{(l)}_{nj})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uSYDthU8EeS"
   },
   "source": [
    "# Final comments on backpropagation and what is next\n",
    "\n",
    "So optimizing a feed-forward neural network with gradient descent is pretty straightforward!\n",
    "1. You forward propagate with the rules you derived in exercise c), storing the results for the $a$s and $z$s.\n",
    "2. Then you run the backward propagation recursion for the $\\delta$s taking $\\delta^{(L)}_j$ from exercise i) and using the recursion from exercise j).\n",
    "3. You get the gradients from the expression in exercise j).\n",
    "3. Finally you update the weights of the network according to the gradients, and the network should now have better performance on the training set.\n",
    "\n",
    "If you wonder why the forward recursion for $a$ and $z$ is non-linear and the backward recursion for $\\delta$ is linear then remember that the derivative is a [linear operator](https://en.wikipedia.org/wiki/Operator_(mathematics)).\n",
    "\n",
    "So what comes next? The next two weeks we will stay with the FFNN model. You will implement the forward and backward pass in NumPy yourself next week and in two weeks we will see how we can do this in PyTorch. One of the features of modern algebra frameworks like PyTorch and TensorFlow is that we only need to bother about the forward pass. We have finally \"taught\" computers to differentiate. ;-)  \n",
    "\n",
    "In three weeks+ we will look at other generalisations of the FFNN. They all build upon the same principles as for the FFNN so the same insights apply. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.13 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.9 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1_1_FNN_Pen_and_Paper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
